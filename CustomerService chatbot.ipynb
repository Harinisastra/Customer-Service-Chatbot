{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_DwmAdQ3pJ0"
      },
      "outputs": [],
      "source": [
        "!pip install speechrecognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtts"
      ],
      "metadata": {
        "id": "UVPruUlo3ui_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "eXSPn132316-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import json\n",
        "import nltk\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gtts import gTTS\n",
        "from io import BytesIO\n",
        "import tensorflow as tf\n",
        "import IPython.display as ipd\n",
        "import speech_recognition as sr\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Flatten, Dense, GlobalMaxPool1D"
      ],
      "metadata": {
        "id": "Rq5HZ1uC37OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Package sentence tokenizer\n",
        "nltk.download('punkt')\n",
        "# Package lemmatization\n",
        "nltk.download('wordnet')\n",
        "# Package multilingual wordnet data\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "Plve2I0k3_sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "with open('data.json') as content:\n",
        "  data1 = json.load(content)\n",
        "\n",
        "# Gets all data into a list\n",
        "tags = [] # data tag\n",
        "inputs = [] # input data or patterns\n",
        "responses = {} # data responses\n",
        "words = [] # Word data\n",
        "classes = [] # Class or Tag Data\n",
        "documents = [] # Document Sentence Data\n",
        "ignore_words = ['?', '!'] # Ignores special character tags\n",
        "\n",
        "for intent in data1['intents']:\n",
        "  responses[intent['tag']]=intent['responses']\n",
        "  for lines in intent['patterns']:\n",
        "    inputs.append(lines)\n",
        "    tags.append(intent['tag'])\n",
        "    for pattern in intent['patterns']:\n",
        "      w = nltk.word_tokenize(pattern)\n",
        "      words.extend(w)\n",
        "      documents.append((w, intent['tag']))\n",
        "      # add to our classes list\n",
        "      if intent['tag'] not in classes:\n",
        "        classes.append(intent['tag'])\n",
        "\n",
        "# Convert json data into dataframe\n",
        "data = pd.DataFrame({\"patterns\":inputs, \"tags\":tags})"
      ],
      "metadata": {
        "id": "0sblLhNe4Fte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "noejDMXz4Jhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Punctuations\n",
        "data['patterns'] = data['patterns'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])\n",
        "data['patterns'] = data['patterns'].apply(lambda wrd: ''.join(wrd))"
      ],
      "metadata": {
        "id": "A8oU8G1q4Ob4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "print (len(words), \"unique lemmatized words\", words)"
      ],
      "metadata": {
        "id": "8zh5rFgg4SsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "print (len(classes), \"classes\", classes)"
      ],
      "metadata": {
        "id": "LaxIq0OD4WfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")"
      ],
      "metadata": {
        "id": "nGnpLC984aPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(data['patterns'])\n",
        "train = tokenizer.texts_to_sequences(data['patterns'])\n",
        "train"
      ],
      "metadata": {
        "id": "CsKjKvQ94dVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply padding\n",
        "x_train = pad_sequences(train)\n",
        "print(x_train) # Padding Sequences"
      ],
      "metadata": {
        "id": "yCWzTvI54ibT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the outputs\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(data['tags'])\n",
        "print(y_train) #Label Encodings"
      ],
      "metadata": {
        "id": "IZZUttW74nn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input length\n",
        "input_shape = x_train.shape[1]\n",
        "print(input_shape)"
      ],
      "metadata": {
        "id": "61-4g9eT4tgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define vocabulary\n",
        "vocabulary = len(tokenizer.word_index)\n",
        "print(\"number of unique words : \", vocabulary)\n",
        "\n",
        "# output length\n",
        "output_length = le.classes_.shape[0]\n",
        "print(\"output length: \", output_length)"
      ],
      "metadata": {
        "id": "cDaS2oZf4xx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(words, open('words.pkl','wb'))\n",
        "pickle.dump(classes, open('classes.pkl','wb'))"
      ],
      "metadata": {
        "id": "sJKnw12641bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(le, open('le.pkl','wb'))\n",
        "pickle.dump(tokenizer, open('tokenizers.pkl','wb'))"
      ],
      "metadata": {
        "id": "y71jLWry45Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to perform NLU and sentiment analysis\n",
        "def perform_nlu_sentiment(user_input):\n",
        "    # Tokenize the user input into individual words\n",
        "    words = word_tokenize(user_input.lower())\n",
        "\n",
        "    # Remove stop words from the tokenized words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Perform part-of-speech tagging to identify the type of each word\n",
        "    tagged_words = nltk.pos_tag(filtered_words)\n",
        "\n",
        "    # Identify named entities in the user input\n",
        "    named_entities = nltk.ne_chunk(tagged_words)\n",
        "\n",
        "    # Perform sentiment analysis on the user input\n",
        "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = sentiment_analyzer.polarity_scores(user_input)\n",
        "\n",
        "    # Identify the intent of the user input\n",
        "    intent = identify_intent(tagged_words)\n",
        "\n",
        "    # Return the results of the NLU and sentiment analysis\n",
        "    return {\n",
        "        'words': words,\n",
        "        'filtered_words': filtered_words,\n",
        "        'tagged_words': tagged_words,\n",
        "        'named_entities': named_entities,\n",
        "        'sentiment_scores': sentiment_scores,\n",
        "        'intent': intent\n",
        "    }\n",
        "\n",
        "# Define a function to identify the intent of the user input\n",
        "def identify_intent(tagged_words):\n",
        "    # Implement your own logic here to identify the intent based on the part-of-speech tags\n",
        "    # This can involve using a rule-based approach or a machine learning model\n",
        "    # For example, you could use a decision tree classifier to classify the intent based on the tags\n",
        "    # Return the identified intent\n",
        "    return 'unknown'"
      ],
      "metadata": {
        "id": "CAdNRYWR49a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model\n",
        "i = Input(shape=(input_shape,))\n",
        "x = Embedding(vocabulary+1,10)(i) # Layer Embedding\n",
        "x = LSTM(7, return_sequences=True)(x) # Layer Long Short Term Memory\n",
        "x = Flatten()(x) # Layer Flatten\n",
        "x = Dense(output_length, activation=\"softmax\")(x) # Layer Dense\n",
        "model  = Model(i,x)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "xmYLnksu5C0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "AYqQt3Ou5GSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "train = model.fit(x_train, y_train, epochs=400)"
      ],
      "metadata": {
        "id": "rfzt0ZI45JXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Input Chat\n",
        "while True:\n",
        "    texts_p = []\n",
        "    prediction_input = input('You: ')\n",
        "\n",
        "    # Remove punctuation and convert to lowercase\n",
        "    prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]\n",
        "    prediction_input = ''.join(prediction_input)\n",
        "    texts_p.append(prediction_input)\n",
        "\n",
        "    # Tokenization and Padding\n",
        "    prediction_input = tokenizer.texts_to_sequences(texts_p)\n",
        "    prediction_input = np.array(prediction_input).reshape(-1)\n",
        "    prediction_input = pad_sequences([prediction_input], input_shape)\n",
        "\n",
        "    # Get the output from the model\n",
        "    output = model.predict(prediction_input)\n",
        "    output = output.argmax()\n",
        "\n",
        "    # Find responses according to tag data and play voice bots\n",
        "    response_tag = le.inverse_transform([output])[0]\n",
        "    print(\"🤖 ChatBot:\", random.choice(responses[response_tag]))\n",
        "\n",
        "    # Generate speech response\n",
        "    tts = gTTS(random.choice(responses[response_tag]), lang='id')\n",
        "    tts.save('ChatBot.wav')\n",
        "\n",
        "    # Play the generated speech response\n",
        "    ipd.display(ipd.Audio('ChatBot.wav', autoplay=True))\n",
        "\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Exit condition\n",
        "    # Exit condition\n",
        "    if response_tag.lower() == \"goodbye\" or str(prediction_input[0]).lower() == \"exit\":\n",
        "      break\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQi6Y5el5SFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}